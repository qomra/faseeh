name: maajim
dataset: mysam/masael_chat
devices:
- 1
actions:
# loading full dataset
- id: load_dataset
  type: load_dataset
  status: always
# training tokenizer or loading it if trained
- id: llama_tokenizer
  type: train_load_tokenizer
  kind: auto
  path: /home/jalalirs/Documents/code/arabi/maknaz/maknaz_/model/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08
  vocab_size: 128256
  status: always

# sft-training on the model trained based on the 10 samples dataset
- id: sft_training
  type: sft
  pretrained_model_kind: auto
  pretrained_model_ckpt: /home/jalalirs/Documents/code/arabi/maknaz/maknaz_/model/models--meta-llama--Llama-3.2-1B/snapshots/4e20de362430cd3b72f300e6b0f18e50e7166e08
  tokenizer_id: llama_tokenizer
  sample_size: 0.90
  path: /home/jalalirs/Documents/code/arabi/maknaz/maknaz_/model/mysam/oryx-2.0-1B-Instruct-Llama

  # this is sft config
  sft_config:
    seed: 42
    max_steps: -1
    save_strategy: no
    num_train_epochs: 1
    max_seq_length: 8192
    save_total_limit:
    learning_rate: 2.0e-05
    logging_strategy: steps
    lr_scheduler_type: cosine
    output_dir: /home/jalalirs/Documents/code/arabi/maknaz/maknaz_/model/mysam/oryx-2.0-1B-Instruct-Llama
    per_device_eval_batch_size: 1
    per_device_train_batch_size: 1
    logging_steps: 10
  status: always
